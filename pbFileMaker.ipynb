{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmuCyDKj6ohtcOoZ0Yc4O5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangHoon-Chae/ProblemSolving/blob/main/pbFileMaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dataDivide as dD\n",
        "#import dataExtractFunc as dataExt\n",
        "import dataExtractFunc2 as dataExt2\n",
        "import dataExtractFunc_AG_aug_userID as dataExt3\n",
        "import sys, os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "sys.path.append(os.pardir)\n",
        "from keras.optimizers import SGD\n",
        "# import dataExtractFunc as dataExt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "from tensorflow.python.tools import freeze_graph\n",
        "from tensorflow.python.tools import optimize_for_inference_lib\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 5\n",
        "epochs = 30\n",
        "romIntegral = 0\n",
        "\n",
        "#img_cols means the size of time-window\n",
        "img_rows, img_cols = 6, 90\n",
        "time_window = 30\n",
        "\n",
        "def shuffle_dataset(x, t):\n",
        "    \"\"\"\n",
        "    데이터셋을 뒤섞는다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 훈련 데이터\n",
        "    t : 정답 레이블\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x, t : 뒤섞은 훈련 데이터와 정답 레이블\n",
        "    \"\"\"\n",
        "    permutation = np.random.permutation(x.shape[0])\n",
        "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
        "    t = t[permutation]\n",
        "\n",
        "    return x, t\n",
        "\n",
        "\n",
        "\n",
        "def extract_accelerometer_data(file_path):\n",
        "    ax, ay, az = [], [], []  # 각각의 데이터를 저장할 배열 초기화\n",
        "\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "        # 특정 키워드 이후 데이터 읽기 시작\n",
        "        start_reading = False\n",
        "        for line in lines:\n",
        "            line = line.strip('')  # 줄 끝 공백 제거\n",
        "#            print(\"Processing line: %s\" %line)  # 디버그: 현재 처리 중인 줄\n",
        "\n",
        "            if line == \"End of file\":\n",
        "                print(\"Reached end of file.\")\n",
        "                break  # 파일 끝이면 종료\n",
        "\n",
        "            if start_reading:\n",
        "                # 데이터를 공백 기준으로 나눠서 ax, ay, az로 분리\n",
        "                try:\n",
        "                    x, y, z = map(float, line.split(\"\\t\"))\n",
        "                    ax.append(x)\n",
        "                    ay.append(y)\n",
        "                    az.append(z)\n",
        "                except ValueError:\n",
        "                    print(\"잘못된 데이터 형식: %s\" % line)\n",
        "#                    print(f\"잘못된 데이터 형식: {line}\")\n",
        "\n",
        "            if line == \"Accelerometer\\n\":\n",
        "                print(\"Found 'accelerometer' keyword.\")\n",
        "                start_reading = True  # accelerometer 이후부터 읽기 시작\n",
        "\n",
        "    return ax, ay, az\n",
        "\n",
        "\n",
        "# 파일 경로 설정\n",
        "file_path = 'sensorData2024-11-27-06-16-07.txt'\n",
        "\n",
        "# 함수 호출 및 결과 저장\n",
        "ax, ay, az = extract_accelerometer_data(file_path)\n",
        "\n",
        "gx, gy, gz = [], [], []\n",
        "gx = ax\n",
        "gy = ay\n",
        "gz= az\n",
        "\n",
        "ax = np.asarray(ax);ay = np.asarray(ay);az = np.asarray(az);\n",
        "gx = np.asarray(gx);gy = np.asarray(gy);gz = np.asarray(gz);\n",
        "\n",
        "ax = ax.reshape(-1,len(ax));ay = ay.reshape(-1,len(ay));az = az.reshape(-1, len(az));\n",
        "gx = gx.reshape(-1,len(gx));gy = gy.reshape(-1,len(gy));gz = gz.reshape(-1, len(gz));\n",
        "\n",
        "data = np.concatenate((ax,ay,az, gx, gy, gz), axis = 0);\n",
        "\n",
        "# data = data[:, 0:data.shape[1]-(data.shape[1])%img_cols]\n",
        "# data = np.hsplit(data, int(data.shape[1]/img_cols))\n",
        "# data = np.asarray(data)\n",
        "\n",
        "x_noExer, t_noExer = dD.dataDivideByTimdWindow(data, 30, img_cols, 0, 5);\n",
        "x_flex, t_flex = dD.dataDivideByTimdWindow(data, 30, img_cols, 1, 5);\n",
        "x_wall, t_wall = dD.dataDivideByTimdWindow(data, 30, img_cols, 2, 5);\n",
        "x_actSca, t_actSca = dD.dataDivideByTimdWindow(data, 30, img_cols, 3, 5);\n",
        "x_towel, t_towel = dD.dataDivideByTimdWindow(data, 30, img_cols, 4, 5);\n",
        "\n",
        "x_train_result = np.concatenate((x_noExer, x_flex, x_wall, x_actSca,  x_towel))\n",
        "t_train_result = np.concatenate((t_noExer, t_flex, t_wall, t_actSca, t_towel))\n",
        "x_train_result, t_train_result = shuffle_dataset(x_train_result, t_train_result)\n",
        "\n",
        "x_test_result = x_train_result[0:30]\n",
        "t_test_result = t_train_result[0:30]\n",
        "\n",
        "x_train_result = x_train_result[31:len(x_train_result)]\n",
        "t_train_result = t_train_result[31:len(t_train_result)]\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train_result = x_train_result.reshape(x_train_result.shape[0], 1, img_rows, img_cols)\n",
        "    x_test_result = x_test_result.reshape(x_test_result.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train_result = x_train_result.reshape(x_train_result.shape[0], img_rows, img_cols, 1)\n",
        "    x_test_result = x_test_result.reshape(x_test_result.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "print('x_train shape:', x_train_result.shape)\n",
        "print(x_train_result.shape[0], 'train samples')\n",
        "print(x_test_result.shape[0], 'test samples')\n",
        "\n",
        "#t_train_result = keras.utils.to_categorical(t_train_result, num_classes)\n",
        "#t_test_result = keras.utils.to_categorical(t_test_result, num_classes)\n",
        "#%%\n",
        "# set learning phase to 0\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(2, 1),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (2, 1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_result, t_train_result,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test_result, t_test_result))\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "\n",
        "# # summarize history for accuracy\n",
        "# plt.plot(history.history['acc'])\n",
        "# plt.plot(history.history['val_acc'])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # summarize history for loss\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "score = model.evaluate(x_test_result, t_test_result, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "MODEL_NAME = \"keras_test\"\n",
        "\n",
        "tf.saved_model.save(model, 'out/' + MODEL_NAME)\n",
        "\n",
        "del model  # deletes the existing model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0b0DP7wT2x9",
        "outputId": "511d9a5b-a111-499b-fa67-25a198878723"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 'accelerometer' keyword.\n",
            "잘못된 데이터 형식: End of file\n",
            "\n",
            "x_train shape: (64, 6, 90, 1)\n",
            "64 train samples\n",
            "30 test samples\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2031 - loss: 1.8373 - val_accuracy: 0.2000 - val_loss: 1.9755\n",
            "Epoch 2/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.2031 - loss: 1.8202 - val_accuracy: 0.2000 - val_loss: 1.9601\n",
            "Epoch 3/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.2031 - loss: 1.8041 - val_accuracy: 0.2000 - val_loss: 1.9453\n",
            "Epoch 4/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.2031 - loss: 1.7886 - val_accuracy: 0.2000 - val_loss: 1.9309\n",
            "Epoch 5/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - accuracy: 0.2031 - loss: 1.7736 - val_accuracy: 0.2000 - val_loss: 1.9171\n",
            "Epoch 6/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.2031 - loss: 1.7595 - val_accuracy: 0.2000 - val_loss: 1.9039\n",
            "Epoch 7/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - accuracy: 0.2031 - loss: 1.7461 - val_accuracy: 0.2000 - val_loss: 1.8916\n",
            "Epoch 8/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.2031 - loss: 1.7336 - val_accuracy: 0.2000 - val_loss: 1.8798\n",
            "Epoch 9/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.2031 - loss: 1.7218 - val_accuracy: 0.2000 - val_loss: 1.8689\n",
            "Epoch 10/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.2031 - loss: 1.7107 - val_accuracy: 0.2000 - val_loss: 1.8584\n",
            "Epoch 11/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 0.2031 - loss: 1.7001 - val_accuracy: 0.2000 - val_loss: 1.8485\n",
            "Epoch 12/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.2031 - loss: 1.6901 - val_accuracy: 0.2000 - val_loss: 1.8392\n",
            "Epoch 13/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 647ms/step - accuracy: 0.2031 - loss: 1.6807 - val_accuracy: 0.2000 - val_loss: 1.8307\n",
            "Epoch 14/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 0.2031 - loss: 1.6719 - val_accuracy: 0.2000 - val_loss: 1.8228\n",
            "Epoch 15/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step - accuracy: 0.2031 - loss: 1.6637 - val_accuracy: 0.2000 - val_loss: 1.8142\n",
            "Epoch 16/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699ms/step - accuracy: 0.2031 - loss: 1.6547 - val_accuracy: 0.2000 - val_loss: 1.8065\n",
            "Epoch 17/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 782ms/step - accuracy: 0.2031 - loss: 1.6466 - val_accuracy: 0.2000 - val_loss: 1.7995\n",
            "Epoch 18/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781ms/step - accuracy: 0.2031 - loss: 1.6391 - val_accuracy: 0.2000 - val_loss: 1.7934\n",
            "Epoch 19/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step - accuracy: 0.2031 - loss: 1.6323 - val_accuracy: 0.2000 - val_loss: 1.7880\n",
            "Epoch 20/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - accuracy: 0.2031 - loss: 1.6261 - val_accuracy: 0.2000 - val_loss: 1.7833\n",
            "Epoch 21/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.2031 - loss: 1.6204 - val_accuracy: 0.2000 - val_loss: 1.7792\n",
            "Epoch 22/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - accuracy: 0.2031 - loss: 1.6153 - val_accuracy: 0.2000 - val_loss: 1.7758\n",
            "Epoch 23/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.2031 - loss: 1.6107 - val_accuracy: 0.2000 - val_loss: 1.7730\n",
            "Epoch 24/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.2031 - loss: 1.6066 - val_accuracy: 0.1333 - val_loss: 1.7707\n",
            "Epoch 25/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.2344 - loss: 1.6028 - val_accuracy: 0.1333 - val_loss: 1.7688\n",
            "Epoch 26/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.2344 - loss: 1.5994 - val_accuracy: 0.1333 - val_loss: 1.7674\n",
            "Epoch 27/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.2344 - loss: 1.5964 - val_accuracy: 0.1333 - val_loss: 1.7663\n",
            "Epoch 28/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - accuracy: 0.2344 - loss: 1.5936 - val_accuracy: 0.1333 - val_loss: 1.7656\n",
            "Epoch 29/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - accuracy: 0.2344 - loss: 1.5911 - val_accuracy: 0.1333 - val_loss: 1.7653\n",
            "Epoch 30/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.2344 - loss: 1.5889 - val_accuracy: 0.1333 - val_loss: 1.7652\n",
            "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])\n",
            "Test loss: 1.765203833580017\n",
            "Test accuracy: 0.13333334028720856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S6YSjPUn-JDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_result.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXqdjcNyKQdG",
        "outputId": "0e425cff-a5e6-4dfb-aaeb-313add3f72ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        }
      ]
    }
  ]
}